###############################
# Generic slurm.conf for NOAA #
###############################

DisableRootJobs=YES

MaxJobCount=500000
# four days
#MinJobAge=345600

ProctrackType=proctrack/cgroup

#PrologFlags=contain

SlurmUser=slurm
#StateSaveLocation=/var/spool/slurmctld

# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0

# SCHEDULING
#DefMemPerCPU=0
#MaxMemPerCPU=0
FastSchedule=1
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
#Changes to this requires a slurmctd restart
SelectType=select/cons_res
#SelectTypeParameters=CR_Core_Memory,CR_ONE_TASK_PER_CORE
#Per ( https://slurm.schedmd.com/cpu_management.html#Example) Example 12:
#SelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK
TaskPlugin=task/affinity,task/cgroup

# JOB PRIORITY
#PriorityFlags=FAIR_TREE

# Activate the Multi-Factor Job Priority Plugin
PriorityType=priority/multifactor

# Apply a decay over 14 days (14-0)
PriorityDecayHalfLife=14-0

# PriorityCalcPeriod=
PriorityFavorSmall=NO

#PriorityWeightJobSize=

# The job's age factor reaches 1.0 after waiting in the queue for 7 days (10080 min)
# Reset usage after ....
PriorityUsageResetPeriod=NONE

# This is expected to vary by site:
PriorityMaxAge=2-0

# Half of the Fairshare. After MaxAge, this moves a dominating user back to a balanced users priority, and gets their stuff to run again.
PriorityWeightAge=5000000

PriorityWeightFairshare=10000000

# PriorityWeightPartition=

# 100-CPU job effectively gets a ~9 hour head-start, given the PriorityMaxAge and PriorityWeightAge values above
PriorityWeightTRES=CPU=10000

PriorityWeightQOS=1000000000
# QOS priorities should range from 0 - 10
# 0 -
# 10 - windfall
# 20 - normal/batch
# 30 - debug
# 40 - urgent/persistent

# 100 - placeholder qos
# sacctmgr add qos maximum-qos-normalization maxsubmit=0 priority=100
# look at using usagefactor to charge at a higher rate for access to "premium" qos's

# LOGGING AND ACCOUNTING
AccountingStorageEnforce=safe,qos,limits,associations
AccountingStorageType=accounting_storage/slurmdbd

# Removed in 21.08
#AccountingStoreJobComment=YES
AccountingStoreFlags=job_comment
#DebugFlags=
#JobCompHost=slurmdbd01
#JobCompLoc=slurm_acct_db
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/filetxt
#JobCompType=jobcomp/mysql
#JobCompUser=
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/cgroup

# Includes are also possible
# Include=/path/to/file

# 18 hours
MinJobAge=64800

# Turn off Slurm Array jobs
MaxArraySize=0

# Allow JobFile Append
JobFileAppend=1

## PROD ##
ControlMachine=slurm03

## BACKUP ##
BackupController=slurm04

# NOTE - that due to the cross-grid solution, it is not recommended to use pure HA

# Accoring to this:
# Will allow for larger job scripts to be specified
# Default is 4MB
# Must be in bytes
# Here we are limiting to 20MB
#
# Prolog nohold is for TESTING ONLY
# Will be removed upon production
SchedulerParameters=max_script_size=5000000,kill_invalid_depend

AccountingStorageHost=slurmdbd01
AccountingStorageTRES=billing,cpu,energy,mem,node

# CM 3868
MessageTimeout=20

ClusterName=batch


## 20.02 change for Federation
#AccountingStorageExternalHost=

#JobCompLoc=slurm_acct_db

HealthCheckInterval=90
HealthCheckProgram=/home/bin/node_check_slurm
HealthCheckNodeState=ANY

#MailDomain=
MpiDefault=none

#PrologFlags=x11,contain

Prolog=/home/bin/slurm.prolog
Epilog=/home/bin/slurm.epilog

TaskProlog=/home/bin/slurm.task_prolog
TaskEpilog=/home/bin/slurm.task_epilog

SlurmctldDebug=debug
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmctldLogFile=/var/log/slurm/slurmctld.log

SlurmdDebug=debug
SlurmdPidFile=/var/run/slurm/slurmd.pid
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmdSpoolDir=/var/spool/slurmd

## Clustered Slurm controller configuration
#StateSaveLocation=/var/spool/slurmctld
StateSaveLocation=/apps/slurm/spool/slurmctld

#TmpFS=/tmp

SelectTypeParameters=CR_CPU,CR_LLN

# Change to Fairshare priority Tree to "classic" implementation
PriorityFlags=NO_FAIR_TREE

# Evaluation for Features
# NodeName=Default State=UNKNOWN CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Feature=bigvftmp
# NodeName=Default State=UNKNOWN CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Feature=bigmem

# Evaluation for GRES
# NodeName=Default State=UNKNOWN CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Gres=vftmp:4T
# NodeName=Default State=UNKNOWN CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Gres=vftmp:20T


# Here we will establish a new Default value and then assign the remaining nodes as normal
NodeName=Default State=UNKNOWN CPUs=8

## Partitions
PartitionName=DEFAULT State=DOWN DefaultTime=1:00:00 MaxTime=INFINITE MaxNodes=1